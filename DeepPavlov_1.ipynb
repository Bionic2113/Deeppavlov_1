{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41af9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 17:57:29.616 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_ru_convers_distilrubert_2L.tar.gz download because of matching hashes\n",
      "2022-11-18 17:57:30.772 ERROR in 'deeppavlov.core.common.log_events'['log_events'] at line 49: Failed to import SummaryWriter from torch.utils.tensorboard. Failed to initialize Tensorboard logger. Install appropriate Pytorch version to use this logger or remove tensorboard_log_dir parameter from the train parameters list in the configuration file.\n",
      "[nltk_data] Downloading package punkt to /Users/a1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/a1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /Users/a1/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /Users/a1/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2022-11-18 17:57:41.962 INFO in 'deeppavlov.models.torch_bert.torch_transformers_squad'['torch_transformers_squad'] at line 273: From pretrained DeepPavlov/distilrubert-tiny-cased-conversational.\n",
      "Some weights of the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-11-18 17:57:45.500 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 153: Load path /Users/a1/.deeppavlov/models/squad_ru_convers_distilrubert_2L/model is given.\n",
      "2022-11-18 17:57:45.500 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 160: Load path /Users/a1/.deeppavlov/models/squad_ru_convers_distilrubert_2L/model.pth.tar exists.\n",
      "2022-11-18 17:57:45.500 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 161: Initializing `TorchTransformersSquad` from saved.\n",
      "2022-11-18 17:57:45.500 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 168: Loading weights from /Users/a1/.deeppavlov/models/squad_ru_convers_distilrubert_2L/model.pth.tar.\n",
      "2022-11-18 17:57:46.567 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized! Model summary:\n",
      " DistilBertForQuestionAnswering(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "504it [02:43,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 5036, \"metrics\": {\"squad_v2_f1\": 65.2044, \"squad_v2_em\": 44.5195, \"squad_v1_f1\": 65.3992, \"squad_v1_em\": 44.6525}, \"time_spent\": \"0:02:44\"}}\n"
     ]
    }
   ],
   "source": [
    "#from deeppavlov import evaluate_model\n",
    "\n",
    "#model = evaluate_model('squad_ru_convers_distilrubert_2L', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94cd84f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:02:14.575 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_ru_convers_distilrubert_2L.tar.gz download because of matching hashes\n",
      "2022-11-18 18:02:25.485 INFO in 'deeppavlov.models.torch_bert.torch_transformers_squad'['torch_transformers_squad'] at line 273: From pretrained DeepPavlov/distilrubert-tiny-cased-conversational.\n",
      "Some weights of the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-11-18 18:02:29.788 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 153: Load path /Users/a1/.deeppavlov/models/squad_ru_convers_distilrubert_2L/model is given.\n",
      "2022-11-18 18:02:29.789 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 160: Load path /Users/a1/.deeppavlov/models/squad_ru_convers_distilrubert_2L/model.pth.tar exists.\n",
      "2022-11-18 18:02:29.789 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 161: Initializing `TorchTransformersSquad` from saved.\n",
      "2022-11-18 18:02:29.789 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 168: Loading weights from /Users/a1/.deeppavlov/models/squad_ru_convers_distilrubert_2L/model.pth.tar.\n",
      "2022-11-18 18:02:31.79 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized! Model summary:\n",
      " DistilBertForQuestionAnswering(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model\n",
    "\n",
    "model = build_model('squad_ru_convers_distilrubert_2L', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943bad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=model(['Космохимия изучает химический состав космических тел, законы распространённости и распределения химических элементов во Вселенной, процессы сочетания и миграции атомов при образовании космического вещества. Иногда выделяют ядерную космохимию, изучающую процессы радиоактивного распада и изотопный состав космических тел. Нуклеогенез в рамках космохимии не рассматривается.'],\n",
    "             ['Что изучает космохимия?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8187435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['химический состав космических тел']\n"
     ]
    }
   ],
   "source": [
    "print(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc099a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7/8 (ya zabi)l",
   "language": "python",
   "name": "downgrade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
